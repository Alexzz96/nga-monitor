# NGA Monitor é¡¹ç›® - è¯¦ç»†ä»£ç å®¡æŸ¥æŠ¥å‘Š

**å®¡æŸ¥æ—¥æœŸ**: 2026-02-05  
**å®¡æŸ¥èŒƒå›´**: src/web/app.py, src/monitor.py, src/nga_crawler.py, src/discord_sender.py, src/ai_analyzer.py, src/db/models.py, src/schedule_manager.py  
**å®¡æŸ¥é‡ç‚¹**: ä»£ç é‡å¤ã€æ€§èƒ½ã€é”™è¯¯å¤„ç†ã€èµ„æºæ³„æ¼ã€å¼‚æ­¥/åŒæ­¥æ··ç”¨ã€ç¡¬ç¼–ç ã€æ—¥å¿—ã€å®‰å…¨é£é™©ã€å¯ç»´æŠ¤æ€§

---

## ç›®å½•
1. [ä¸¥é‡é—®é¢˜ï¼ˆéœ€ç«‹å³ä¿®å¤ï¼‰](#1-ä¸¥é‡é—®é¢˜éœ€ç«‹å³ä¿®å¤)
2. [é«˜ä¼˜å…ˆçº§é—®é¢˜](#2-é«˜ä¼˜å…ˆçº§é—®é¢˜)
3. [ä¸­ä¼˜å…ˆçº§é—®é¢˜](#3-ä¸­ä¼˜å…ˆçº§é—®é¢˜)
4. [ä½ä¼˜å…ˆçº§é—®é¢˜ï¼ˆä¼˜åŒ–å»ºè®®ï¼‰](#4-ä½ä¼˜å…ˆçº§é—®é¢˜ä¼˜åŒ–å»ºè®®)
5. [ä»£ç å¼‚å‘³](#5-ä»£ç å¼‚å‘³)
6. [å®‰å…¨å®¡è®¡ç»“æœ](#6-å®‰å…¨å®¡è®¡ç»“æœ)

---

## 1. ä¸¥é‡é—®é¢˜ï¼ˆéœ€ç«‹å³ä¿®å¤ï¼‰

### 1.1 API Key æ³„éœ²åˆ°æ—¥å¿—ï¼ˆå®‰å…¨é£é™©ï¼‰

**æ–‡ä»¶**: `src/ai_analyzer.py`  
**è¡Œå·**: 34-35

**é—®é¢˜æè¿°**:  
API Key çš„éƒ¨åˆ†å†…å®¹è¢«è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶ä¸­ï¼Œå³ä½¿åªè®°å½•äº†å‰ç¼€å’Œåç¼€ï¼Œä»å­˜åœ¨å®‰å…¨é£é™©ã€‚æ”»å‡»è€…è·å–æ—¥å¿—åå¯èƒ½æ›´å®¹æ˜“çŒœæµ‹æˆ–ç¤¾å·¥è·å–å®Œæ•´å¯†é’¥ã€‚

**å½“å‰ä»£ç **:
```python
if self.api_key:
    logger.info(f"[AIAnalyzer] API Key å·²é…ç½®: {bool(self.api_key)}")
    logger.info(f"[AIAnalyzer] API Key: {self.api_key[:10]}...{self.api_key[-4:]}")  # æ³„éœ²é£é™©ï¼
```

**ä¿®å¤ä»£ç **:
```python
if self.api_key:
    logger.info(f"[AIAnalyzer] API Key å·²é…ç½®: {bool(self.api_key)}")
    # ä¸è¦è®°å½•ä»»ä½• API Key å†…å®¹
    logger.debug(f"[AIAnalyzer] API Key é•¿åº¦: {len(self.api_key)}")  # ä»…è®°å½•é•¿åº¦ç”¨äºè°ƒè¯•
```

**é£é™©ç­‰çº§**: ğŸ”´ é«˜å±  
**ä¿®å¤ä¼˜å…ˆçº§**: ç«‹å³

---

### 1.2 åŒæ­¥ HTTP è°ƒç”¨é˜»å¡å¼‚æ­¥äº‹ä»¶å¾ªç¯ï¼ˆæ€§èƒ½é—®é¢˜ï¼‰

**æ–‡ä»¶**: 
- `src/discord_sender.py` ç¬¬ 88-92 è¡Œ
- `src/ai_analyzer.py` ç¬¬ 53-84 è¡Œ  
- `src/web/app.py` ç¬¬ 412-420 è¡Œ

**é—®é¢˜æè¿°**:  
åœ¨å¼‚æ­¥å‡½æ•°ä¸­ä½¿ç”¨ `requests` åº“è¿›è¡Œ HTTP è°ƒç”¨ä¼šé˜»å¡æ•´ä¸ªäº‹ä»¶å¾ªç¯ã€‚è¿™æ„å‘³ç€å½“ç­‰å¾…ç½‘ç»œå“åº”æ—¶ï¼Œå…¶ä»–æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡éƒ½æ— æ³•æ‰§è¡Œã€‚

**å½“å‰ä»£ç ** (`discord_sender.py`):
```python
def send_reply(self, reply):
    # ...
    response = requests.post(
        self.webhook_url,
        json={"embeds": [embed]},
        timeout=30,  # è¿™30ç§’å†…æ•´ä¸ªåº”ç”¨è¢«é˜»å¡ï¼
        headers={'Content-Type': 'application/json'}
    )
```

**ä¿®å¤ä»£ç **:
```python
import aiohttp

class DiscordSender:
    def __init__(self, webhook_url):
        self.webhook_url = webhook_url
    
    async def send_reply(self, reply):
        """å¼‚æ­¥å‘é€å›å¤"""
        # ... æ„å»º embed ...
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.webhook_url,
                json={"embeds": [embed]},
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                return response.status == 204
    
    # ä¿æŒåŒæ­¥ç‰ˆæœ¬ç”¨äºéå¼‚æ­¥ä¸Šä¸‹æ–‡
    def send_reply_sync(self, reply):
        """åŒæ­¥å‘é€å›å¤ï¼ˆå‘åå…¼å®¹ï¼‰"""
        # ... åŸä»£ç  ...
```

**ä¿®å¤ä»£ç ** (`ai_analyzer.py`):
```python
import httpx  # httpx æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥

class AIAnalyzer:
    async def _call_api_async(self, messages: List[Dict]) -> Optional[str]:
        """å¼‚æ­¥è°ƒç”¨ AI API"""
        try:
            async with httpx.AsyncClient(timeout=60) as client:
                response = await client.post(
                    f'{self.api_base}/chat/completions',
                    headers={'Authorization': f'Bearer {self.api_key}'},
                    json={
                        'model': self.model,
                        'messages': messages,
                        'max_tokens': 2000
                    }
                )
                # ... å¤„ç†å“åº” ...
        except httpx.TimeoutException:
            logger.error("[AI API] è¯·æ±‚è¶…æ—¶")
            return None
        except httpx.HTTPError as e:
            logger.error(f"[AI API] HTTP é”™è¯¯: {e}")
            return None
```

**é£é™©ç­‰çº§**: ğŸ”´ é«˜å±  
**ä¿®å¤ä¼˜å…ˆçº§**: ç«‹å³

---

### 1.3 SSRFï¼ˆæœåŠ¡å™¨ç«¯è¯·æ±‚ä¼ªé€ ï¼‰æ¼æ´

**æ–‡ä»¶**: `src/web/app.py`  
**è¡Œå·**: 412-420

**é—®é¢˜æè¿°**:  
`/api/ai/models` ç«¯ç‚¹ç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„ `base_url` å‘èµ· HTTP è¯·æ±‚ï¼Œæ²¡æœ‰éªŒè¯ URL æ˜¯å¦æŒ‡å‘å†…éƒ¨æœåŠ¡ã€‚æ”»å‡»è€…å¯èƒ½åˆ©ç”¨æ­¤æ¼æ´è®¿é—®å†…éƒ¨ç½‘ç»œèµ„æºã€‚

**å½“å‰ä»£ç **:
```python
@app.post("/api/ai/models")
async def get_ai_models(data: dict, db: Session = Depends(get_db)):
    base_url = data.get('base_url', '').rstrip('/')
    api_key = data.get('api_key', '')
    
    # æ²¡æœ‰éªŒè¯ base_urlï¼
    response = requests.get(
        f'{base_url}/models',  # å¯èƒ½æ˜¯ http://localhost:8080/secret
        headers={'Authorization': f'Bearer {api_key}'},
        timeout=10
    )
```

**ä¿®å¤ä»£ç **:
```python
from urllib.parse import urlparse
import ipaddress

ALLOWED_HOSTS = ['api.moonshot.cn', 'api.openai.com', 'api.anthropic.com']
BLOCKED_IP_NETWORKS = [
    ipaddress.ip_network('10.0.0.0/8'),
    ipaddress.ip_network('172.16.0.0/12'),
    ipaddress.ip_network('192.168.0.0/16'),
    ipaddress.ip_network('127.0.0.0/8'),
    ipaddress.ip_network('0.0.0.0/8'),
    ipaddress.ip_network('::1/128'),
]

def is_safe_url(url: str) -> bool:
    """éªŒè¯ URL æ˜¯å¦å®‰å…¨"""
    try:
        parsed = urlparse(url)
        
        # åªå…è®¸ HTTPS
        if parsed.scheme != 'https':
            return False
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å…è®¸çš„ä¸»æœºåˆ—è¡¨ä¸­
        if parsed.hostname not in ALLOWED_HOSTS:
            return False
        
        # è§£æ IP æ£€æŸ¥æ˜¯å¦åœ¨ç§æœ‰ç½‘ç»œ
        try:
            ip = ipaddress.ip_address(parsed.hostname)
            for network in BLOCKED_IP_NETWORKS:
                if ip in network:
                    return False
        except ValueError:
            # æ˜¯åŸŸåä¸æ˜¯ IPï¼Œç»§ç»­æ£€æŸ¥
            pass
        
        return True
    except Exception:
        return False

@app.post("/api/ai/models")
async def get_ai_models(data: dict, db: Session = Depends(get_db)):
    base_url = data.get('base_url', '').rstrip('/')
    
    if not is_safe_url(base_url):
        raise HTTPException(status_code=400, detail="ä¸å®‰å…¨çš„ URL")
    
    # ç»§ç»­å¤„ç†...
```

**é£é™©ç­‰çº§**: ğŸ”´ é«˜å±  
**ä¿®å¤ä¼˜å…ˆçº§**: ç«‹å³

---

## 2. é«˜ä¼˜å…ˆçº§é—®é¢˜

### 2.1 æ•°æ®åº“è¿æ¥èµ„æºæ³„æ¼

**æ–‡ä»¶**: `src/schedule_manager.py`  
**è¡Œå·**: 17, 20-22

**é—®é¢˜æè¿°**:  
åœ¨ `__init__` ä¸­åˆ›å»ºæ•°æ®åº“ä¼šè¯å¹¶åœ¨ `__del__` ä¸­å…³é—­ï¼Œä½† Python çš„ææ„å‡½æ•°ä¸ä¿è¯è°ƒç”¨æ—¶æœºå’Œé¡ºåºï¼Œå¯èƒ½å¯¼è‡´è¿æ¥æ³„æ¼ã€‚

**å½“å‰ä»£ç **:
```python
class ScheduleManager:
    def __init__(self):
        self.db = SessionLocal()  # é•¿æœŸæŒæœ‰è¿æ¥
    
    def __del__(self):
        if hasattr(self, 'db'):
            self.db.close()  # ä¸ä¿è¯è°ƒç”¨ï¼
```

**ä¿®å¤ä»£ç **:
```python
from contextlib import contextmanager

class ScheduleManager:
    """è°ƒåº¦ç®¡ç†å™¨ - ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¨¡å¼"""
    
    @contextmanager
    def _get_db(self):
        """è·å–æ•°æ®åº“ä¼šè¯çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        db = SessionLocal()
        try:
            yield db
            db.commit()
        except Exception:
            db.rollback()
            raise
        finally:
            db.close()
    
    def get_active_rules(self) -> List[ScheduleRule]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„è°ƒåº¦è§„åˆ™"""
        with self._get_db() as db:
            return db.query(ScheduleRule).filter(
                ScheduleRule.enabled == True
            ).order_by(ScheduleRule.priority.desc()).all()
    
    # ç§»é™¤ __del__ æ–¹æ³•
```

**é£é™©ç­‰çº§**: ğŸŸ  é«˜  
**ä¿®å¤ä¼˜å…ˆçº§**: é«˜

---

### 2.2 æ‰¹é‡æ“ä½œç¼ºå¤±å¯¼è‡´æ€§èƒ½é—®é¢˜ï¼ˆN+1æŸ¥è¯¢ï¼‰

**æ–‡ä»¶**: `src/monitor.py`  
**è¡Œå·**: 177-196

**é—®é¢˜æè¿°**:  
å†å²å­˜æ¡£åŠŸèƒ½é€æ¡æŸ¥è¯¢æ•°æ®åº“æ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨ï¼Œç„¶åé€æ¡æ’å…¥ã€‚å½“å¤„ç†å¤§é‡æ•°æ®æ—¶ï¼Œè¿™ä¼šå¯¼è‡´ N+1 æŸ¥è¯¢é—®é¢˜å’Œå¤§é‡å•æ¡æ’å…¥æ“ä½œã€‚

**å½“å‰ä»£ç **:
```python
# å­˜æ¡£åˆ°æ•°æ®åº“
archived_count = 0
skipped_count = 0

for i, reply in enumerate(replies):
    existing = db.query(ReplyArchive).filter(
        ReplyArchive.pid == reply['pid']
    ).first()  # N æ¬¡æŸ¥è¯¢ï¼
    
    if existing:
        skipped_count += 1
        continue
    
    archive = ReplyArchive(...)
    db.add(archive)  # å•æ¡æ·»åŠ 
    archived_count += 1
    
    if archived_count % 50 == 0:
        db.commit()  # é¢‘ç¹æäº¤

db.commit()
```

**ä¿®å¤ä»£ç **:
```python
from sqlalchemy.dialects.sqlite import insert as sqlite_insert

def archive_replies_batch(db, target_id: int, replies: List[Dict]) -> Tuple[int, int]:
    """æ‰¹é‡å­˜æ¡£å›å¤ï¼Œä½¿ç”¨ INSERT OR IGNORE ä¼˜åŒ–"""
    
    # 1. ä¸€æ¬¡æ€§æŸ¥è¯¢æ‰€æœ‰å·²å­˜åœ¨çš„ PID
    reply_pids = [r['pid'] for r in replies if r.get('pid')]
    existing_pids = {
        r.pid for r in db.query(ReplyArchive.pid).filter(
            ReplyArchive.pid.in_(reply_pids)
        ).all()
    }
    
    # 2. è¿‡æ»¤å‡ºæ–°è®°å½•
    new_replies = [
        {
            'target_id': target_id,
            'pid': r['pid'],
            'tid': r.get('tid', ''),
            'topic_title': r.get('topic_title', ''),
            'content_full': r.get('content_full', ''),
            'quote_content': r.get('quote_content', ''),
            'main_content': r.get('main_content', ''),
            'forum': r.get('forum', ''),
            'post_date': r.get('post_date', ''),
            'url': r.get('url', '')
        }
        for r in replies
        if r.get('pid') and r['pid'] not in existing_pids
    ]
    
    # 3. æ‰¹é‡æ’å…¥
    if new_replies:
        # SQLite ä½¿ç”¨ INSERT OR IGNORE é¿å…é‡å¤
        stmt = sqlite_insert(ReplyArchive).values(new_replies)
        stmt = stmt.on_conflict_do_nothing(index_elements=['pid'])
        db.execute(stmt)
        db.commit()
    
    return len(new_replies), len(existing_pids)

# ä½¿ç”¨
archived_count, skipped_count = archive_replies_batch(db, target.id, replies)
```

**æ€§èƒ½å½±å“**: ä» O(N) æ¬¡æŸ¥è¯¢å‡å°‘åˆ° O(1) æ¬¡æŸ¥è¯¢  
**ä¿®å¤ä¼˜å…ˆçº§**: é«˜

---

### 2.3 å¼‚å¸¸é™é»˜å¤„ç†æ©ç›–é”™è¯¯

**æ–‡ä»¶**: `src/nga_crawler.py`  
**è¡Œå·**: 44-46, 136-137, 140-141, 144-145, 150-151, 158-159

**é—®é¢˜æè¿°**:  
å¤šå¤„ä½¿ç”¨ `except Exception: continue` æˆ– `except: pass` é™é»˜å¿½ç•¥æ‰€æœ‰å¼‚å¸¸ï¼Œè¿™ä¼šå¯¼è‡´é”™è¯¯è¢«éšè—ï¼Œéš¾ä»¥è°ƒè¯•ã€‚

**å½“å‰ä»£ç **:
```python
for row in rows:
    try:
        reply = await self._extract_reply(row)
        if reply:
            replies.append(reply)
    except Exception:  # æ•è·æ‰€æœ‰å¼‚å¸¸
        continue  # é™é»˜å¿½ç•¥ - é”™è¯¯æ°¸è¿œä¸ä¼šè¢«å‘ç°ï¼
```

**ä¿®å¤ä»£ç **:
```python
import logging

logger = logging.getLogger(__name__)

for row in rows:
    try:
        reply = await self._extract_reply(row)
        if reply:
            replies.append(reply)
    except Exception as e:
        # è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­å¤„ç†
        logger.warning(f"æå–å›å¤å¤±è´¥: {e}", exc_info=True)
        continue
```

åŒæ ·çš„é—®é¢˜å­˜åœ¨äº `_extract_reply` æ–¹æ³•çš„å¤šä¸ª try-except å—ä¸­ã€‚

**ä¿®å¤ä¼˜å…ˆçº§**: é«˜

---

### 2.4 æŸ¥è¯¢å‚æ•°ç¼ºä¹ä¸Šé™é™åˆ¶ï¼ˆDoSé£é™©ï¼‰

**æ–‡ä»¶**: `src/web/app.py`  
**è¡Œå·**: 231-242, 661-671

**é—®é¢˜æè¿°**:  
å¤šä¸ª API ç«¯ç‚¹æ¥å— `limit` å‚æ•°ä½†æ²¡æœ‰è®¾ç½®ä¸Šé™ï¼Œæ”»å‡»è€…å¯ä»¥è¯·æ±‚è¶…å¤§æ•°é‡çš„æ•°æ®å¯¼è‡´å†…å­˜è€—å°½æˆ–æ•°æ®åº“è´Ÿè½½è¿‡é«˜ã€‚

**å½“å‰ä»£ç **:
```python
@app.get("/api/logs")
async def get_logs(
    level: str = None,
    target_uid: str = None,
    limit: int = 100,  # æ²¡æœ‰æœ€å¤§å€¼é™åˆ¶ï¼
    db: Session = Depends(get_db)
):
    logs = query.order_by(SystemLog.created_at.desc()).limit(limit).all()
    return {"logs": [log.to_dict() for log in logs]}
```

**ä¿®å¤ä»£ç **:
```python
from fastapi import Query

MAX_LOG_LIMIT = 1000
MAX_ARCHIVE_LIMIT = 100

@app.get("/api/logs")
async def get_logs(
    level: str = None,
    target_uid: str = None,
    limit: int = Query(default=100, ge=1, le=MAX_LOG_LIMIT, description="è¿”å›çš„æœ€å¤§è®°å½•æ•°"),
    db: Session = Depends(get_db)
):
    logs = query.order_by(SystemLog.created_at.desc()).limit(limit).all()
    return {"logs": [log.to_dict() for log in logs]}
```

**é£é™©ç­‰çº§**: ğŸŸ  é«˜  
**ä¿®å¤ä¼˜å…ˆçº§**: é«˜

---

### 2.5 æœªéªŒè¯æŸ¥è¯¢ç»“æœæ˜¯å¦ä¸º None

**æ–‡ä»¶**: `src/web/app.py` å¤šå¤„

**é—®é¢˜æè¿°**:  
å¤šå¤„æŸ¥è¯¢æ•°æ®åº“åç›´æ¥ä½¿ç”¨ç»“æœï¼Œæœªæ£€æŸ¥æ˜¯å¦ä¸º Noneï¼Œå¯èƒ½å¯¼è‡´ AttributeErrorã€‚

**å½“å‰ä»£ç ** (ç¬¬ 597-605 è¡Œ):
```python
@app.get("/api/ai/reports/{report_id}")
async def get_report_detail(report_id: int, db: Session = Depends(get_db)):
    report = db.query(AIAnalysisReport).filter(AIAnalysisReport.id == report_id).first()
    # æ²¡æœ‰æ£€æŸ¥ report æ˜¯å¦ä¸º Noneï¼
    result = report.to_dict()  # å¦‚æœ report æ˜¯ Noneï¼Œè¿™é‡Œä¼šæŠ›å‡º AttributeError
    target = db.query(MonitorTarget).filter(MonitorTarget.id == report.target_id).first()
    result['target_name'] = target.name  # å¦‚æœ target æ˜¯ Noneï¼Œè¿™é‡Œä¹Ÿä¼šå‡ºé”™
```

**ä¿®å¤ä»£ç **:
```python
@app.get("/api/ai/reports/{report_id}")
async def get_report_detail(report_id: int, db: Session = Depends(get_db)):
    report = db.query(AIAnalysisReport).filter(AIAnalysisReport.id == report_id).first()
    if not report:
        raise HTTPException(status_code=404, detail="æŠ¥å‘Šä¸å­˜åœ¨")
    
    result = report.to_dict()
    target = db.query(MonitorTarget).filter(MonitorTarget.id == report.target_id).first()
    result['target_name'] = target.name if target else 'æœªçŸ¥ç”¨æˆ·'
    
    return result
```

**ä¿®å¤ä¼˜å…ˆçº§**: é«˜

---

## 3. ä¸­ä¼˜å…ˆçº§é—®é¢˜

### 3.1 æ•°æ®åº“è¿æ¥é‡å¤åˆ›å»º

**æ–‡ä»¶**: `src/monitor.py`  
**è¡Œå·**: 26-30, 79

**é—®é¢˜æè¿°**:  
`get_webhook_from_db` å‡½æ•°æ¯æ¬¡è°ƒç”¨éƒ½åˆ›å»ºæ–°çš„æ•°æ®åº“è¿æ¥ï¼Œè€Œ `check_and_send` ä¸­å·²ç»æœ‰ä¸€ä¸ªæ•°æ®åº“ä¼šè¯ï¼Œåº”è¯¥å¤ç”¨ã€‚

**å½“å‰ä»£ç **:
```python
def get_webhook_from_db():
    db = SessionLocal()
    try:
        return Config.get_webhook(db)
    finally:
        db.close()

async def check_and_send(target_id, force=False):
    db = SessionLocal()
    try:
        # ...
        webhook = get_webhook_from_db()  # åˆåˆ›å»ºäº†ä¸€ä¸ªè¿æ¥ï¼
```

**ä¿®å¤ä»£ç **:
```python
async def check_and_send(target_id, force=False):
    db = SessionLocal()
    try:
        # ...
        webhook = Config.get_webhook(db)  # å¤ç”¨å·²æœ‰è¿æ¥
```

**ä¿®å¤ä¼˜å…ˆçº§**: ä¸­

---

### 3.2 å¼‚å¸¸æ•è·è¿‡äºå®½æ³›

**æ–‡ä»¶**: `src/monitor.py`  
**è¡Œå·**: 115-117

**é—®é¢˜æè¿°**:  
ä½¿ç”¨ `except Exception as e` æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œä¼šéšè—ç¼–ç¨‹é”™è¯¯ï¼Œä½¿è°ƒè¯•å›°éš¾ã€‚

**å½“å‰ä»£ç **:
```python
try:
    # 50+ è¡Œä»£ç  ...
except Exception as e:  # æ•è·ä¸€åˆ‡ï¼ŒåŒ…æ‹¬ AttributeErrorã€TypeError ç­‰ç¼–ç¨‹é”™è¯¯
    logger.error(f"æ£€æŸ¥ç”¨æˆ· {target_id} æ—¶å‡ºé”™: {e}", exc_info=True)
    return {"success": False, "message": str(e)}
```

**ä¿®å¤ä»£ç **:
```python
from sqlalchemy.exc import SQLAlchemyError
from playwright.async_api import TimeoutError as PlaywrightTimeout

async def check_and_send(target_id, force=False):
    db = SessionLocal()
    try:
        # ... ä¸šåŠ¡é€»è¾‘ ...
        pass
    except SQLAlchemyError as e:
        logger.error(f"æ•°æ®åº“é”™è¯¯: {e}", exc_info=True)
        db.rollback()
        return {"success": False, "message": f"æ•°æ®åº“é”™è¯¯: {str(e)}"}
    except PlaywrightTimeout:
        logger.error("çˆ¬å–è¶…æ—¶")
        return {"success": False, "message": "çˆ¬å–ç½‘é¡µè¶…æ—¶"}
    except ValueError as e:
        logger.error(f"æ•°æ®æ ¼å¼é”™è¯¯: {e}")
        return {"success": False, "message": f"æ•°æ®æ ¼å¼é”™è¯¯: {str(e)}"}
    except Exception as e:
        # æœªçŸ¥é”™è¯¯ï¼Œè®°å½•å®Œæ•´å †æ ˆ
        logger.critical(f"æœªé¢„æœŸçš„é”™è¯¯: {e}", exc_info=True)
        return {"success": False, "message": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯"}
    finally:
        db.close()
```

**ä¿®å¤ä¼˜å…ˆçº§**: ä¸­

---

### 3.3 æ­£åˆ™è¡¨è¾¾å¼æœªé¢„ç¼–è¯‘

**æ–‡ä»¶**: `src/nga_crawler.py`  
**è¡Œå·**: 123, 127

**é—®é¢˜æè¿°**:  
æ¯æ¬¡è°ƒç”¨ `_extract_reply` éƒ½é‡æ–°ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œé€ æˆä¸å¿…è¦çš„æ€§èƒ½å¼€é”€ã€‚

**å½“å‰ä»£ç **:
```python
async def _extract_reply(self, row):
    # ...
    img_pattern = r'<img[^\u003e]*data-srcorg="([^"]+)"[^\u003e]*\u003e'
    img_matches = re.findall(img_pattern, postcontent_html)
```

**ä¿®å¤ä»£ç **:
```python
class NgaCrawler:
    # ç±»çº§åˆ«é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
    IMG_PATTERN = re.compile(r'<img[^\u003e]*data-srcorg="([^"]+)"[^\u003e]*\u003e')
    IMG_PATTERN2 = re.compile(r'<img[^\u003e]*src="(https?://[^"]+)"[^\u003e]*\u003e')
    PID_PATTERN = re.compile(r'(\d+)$')
    TID_PATTERN = re.compile(r"tid=(\d+)")
    
    async def _extract_reply(self, row):
        # ...
        img_matches = self.IMG_PATTERN.findall(postcontent_html)
```

**ä¿®å¤ä¼˜å…ˆçº§**: ä¸­

---

### 3.4 é‡å¤ä»£ç  - JSON æå–é€»è¾‘

**æ–‡ä»¶**: `src/ai_analyzer.py`  
**è¡Œå·**: 163-175, 272-281

**é—®é¢˜æè¿°**:  
ä¸¤å¤„ç›¸åŒçš„ JSON æå–é€»è¾‘é‡å¤ã€‚

**å½“å‰ä»£ç **:
```python
# ç¬¬ä¸€æ¬¡
json_start = response.find('{')
json_end = response.rfind('}') + 1
if json_start >= 0 and json_end > json_start:
    json_str = response[json_start:json_end]
    analysis_result = json.loads(json_str)
else:
    analysis_result = json.loads(response)

# ç¬¬äºŒæ¬¡ï¼ˆcompare_users æ–¹æ³•ä¸­ï¼‰
json_start = response.find('{')
json_end = response.rfind('}') + 1
if json_start >= 0 and json_end > json_start:
    json_str = response[json_start:json_end]
    analysis_result = json.loads(json_str)
else:
    analysis_result = {"summary": response[:500]}
```

**ä¿®å¤ä»£ç **:
```python
import json
from typing import Any

def extract_json_from_response(response: str, default_key: str = "summary") -> Any:
    """ä» AI å“åº”ä¸­æå– JSON å†…å®¹"""
    response = response.strip()
    
    # å°è¯•æ‰¾åˆ° JSON å—
    json_start = response.find('{')
    json_end = response.rfind('}') + 1
    
    if json_start >= 0 and json_end > json_start:
        json_str = response[json_start:json_end]
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass
    
    # å°è¯•è§£ææ•´ä¸ªå“åº”
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        # è¿”å›é»˜è®¤å€¼
        return {default_key: response[:500]}

# ä½¿ç”¨
analysis_result = extract_json_from_response(response, default_key="summary")
```

**ä¿®å¤ä¼˜å…ˆçº§**: ä¸­

---

### 3.5 é‡å¤å¯¼å…¥

**æ–‡ä»¶**: `src/nga_crawler.py`  
**è¡Œå·**: ç¬¬ 7 è¡Œå’Œç¬¬ 57 è¡Œ

**é—®é¢˜æè¿°**:  
`asyncio` è¢«å¯¼å…¥äº†ä¸¤æ¬¡ã€‚

**å½“å‰ä»£ç **:
```python
import asyncio  # ç¬¬ 7 è¡Œ

# ...

    async def fetch_history(self, target_url, max_pages=25, delay=2, progress_callback=None):
        import asyncio  # ç¬¬ 57 è¡Œ - é‡å¤å¯¼å…¥
```

**ä¿®å¤**: åˆ é™¤ç¬¬ 57 è¡Œçš„å¯¼å…¥è¯­å¥ã€‚

**ä¿®å¤ä¼˜å…ˆçº§**: ä¸­

---

### 3.6 Playwright æµè§ˆå™¨èµ„æºç®¡ç†

**æ–‡ä»¶**: `src/nga_crawler.py`  
**è¡Œå·**: 28-53

**é—®é¢˜æè¿°**:  
æµè§ˆå™¨å®ä¾‹åˆ›å»ºåï¼Œå¦‚æœåœ¨ `try` å—å†…å‘ç”Ÿå¼‚å¸¸ï¼Œ`browser.close()` å¯èƒ½æ— æ³•æ‰§è¡Œã€‚

**å½“å‰ä»£ç **:
```python
async with async_playwright() as p:
    browser = await p.chromium.launch(headless=True)
    context = await browser.new_context(storage_state=state)
    page = await context.new_page()
    
    try:
        # ... æ“ä½œ ...
    finally:
        await browser.close()
```

**ä¿®å¤ä»£ç **:
```python
async with async_playwright() as p:
    browser = await p.chromium.launch(headless=True)
    try:
        context = await browser.new_context(storage_state=state)
        page = await context.new_page()
        # ... æ“ä½œ ...
    finally:
        await browser.close()
```

**ä¿®å¤ä¼˜å…ˆçº§**: ä¸­

---

### 3.7 å­˜å‚¨çŠ¶æ€æ–‡ä»¶å¼‚å¸¸å¤„ç†ä¸è¶³

**æ–‡ä»¶**: `src/nga_crawler.py`  
**è¡Œå·**: 25

**é—®é¢˜æè¿°**:  
è¯»å– storage state æ–‡ä»¶æ—¶ç¼ºä¹å¼‚å¸¸å¤„ç†ã€‚

**å½“å‰ä»£ç **:
```python
with open(self.storage_state_path, "r") as f:
    state = json.load(f)
```

**ä¿®å¤ä»£ç **:
```python
try:
    with open(self.storage_state_path, "r", encoding='utf-8') as f:
        state = json.load(f)
except FileNotFoundError:
    logger.error(f"Storage state æ–‡ä»¶ä¸å­˜åœ¨: {self.storage_state_path}")
    raise RuntimeError(f"è¯·å…ˆé…ç½® NGA ç™»å½•çŠ¶æ€æ–‡ä»¶: {self.storage_state_path}")
except json.JSONDecodeError as e:
    logger.error(f"Storage state æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
    raise RuntimeError("ç™»å½•çŠ¶æ€æ–‡ä»¶å·²æŸåï¼Œè¯·é‡æ–°å¯¼å‡º")
except Exception as e:
    logger.error(f"è¯»å– storage state å¤±è´¥: {e}")
    raise
```

**ä¿®å¤ä¼˜å…ˆçº§**: ä¸­

---

### 3.8 æ•°æ®åº“ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¼ºå¤±

**æ–‡ä»¶**: `src/db/models.py` å¤šå¤„

**é—®é¢˜æè¿°**:  
å¤šå¤„æ‰‹åŠ¨ç®¡ç†æ•°æ®åº“ä¼šè¯ï¼Œå®¹æ˜“å‡ºé”™ã€‚

**ä¿®å¤ä»£ç **:
```python
from contextlib import contextmanager

@contextmanager
def get_db_session():
    """æ•°æ®åº“ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    db = SessionLocal()
    try:
        yield db
        db.commit()
    except Exception:
        db.rollback()
        raise
    finally:
        db.close()

# ä½¿ç”¨ç¤ºä¾‹
def cleanup_old_logs(days=7):
    with get_db_session() as db:
        cutoff = datetime.now(timezone.utc) - timedelta(days=days)
        return db.query(SystemLog).filter(SystemLog.created_at < cutoff).delete()
```

**ä¿®å¤ä¼˜å…ˆçº§**: ä¸­

---

## 4. ä½ä¼˜å…ˆçº§é—®é¢˜ï¼ˆä¼˜åŒ–å»ºè®®ï¼‰

### 4.1 ç¡¬ç¼–ç é…ç½®å€¼

**ä½ç½®**: 
- `src/nga_crawler.py`: timeout=30000, wait_for_timeout=5000, delay=2
- `src/discord_sender.py`: å­—ç¬¦æˆªæ–­é•¿åº¦
- `src/ai_analyzer.py`: max_tokens=2000, å„ç§å­—ç¬¦ä¸²é•¿åº¦é™åˆ¶

**å»ºè®®**: æå–åˆ°é…ç½®æ–‡ä»¶æˆ–ç±»å¸¸é‡ã€‚

```python
# config.py
class CrawlerConfig:
    PAGE_TIMEOUT = 30000  # ms
    WAIT_TIMEOUT = 5000   # ms
    PAGE_DELAY = 2        # seconds
    MAX_PAGES = 25

class DiscordConfig:
    MAX_TITLE_LENGTH = 256
    MAX_DESCRIPTION_LENGTH = 4096
    MAX_FIELD_LENGTH = 1024
    MAX_CONTENT_PREVIEW = 500
```

**ä¼˜å…ˆçº§**: ä½

---

### 4.2 æ¨¡å—å†…å¯¼å…¥è¯­å¥

**æ–‡ä»¶**: `src/web/app.py`  
**è¡Œå·**: 409, 564

**é—®é¢˜æè¿°**:  
åœ¨å‡½æ•°å†…éƒ¨å¯¼å…¥æ¨¡å—ï¼Œä¸ç¬¦åˆ PEP 8 è§„èŒƒã€‚

**å½“å‰ä»£ç **:
```python
@app.post("/api/ai/models")
async def get_ai_models(...):
    import requests  # åº”è¯¥åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥
```

**ä¿®å¤**: å°†æ‰€æœ‰å¯¼å…¥ç§»åˆ°æ–‡ä»¶é¡¶éƒ¨ã€‚

**ä¼˜å…ˆçº§**: ä½

---

### 4.3 æç¤ºè¯æ¨¡æ¿ç¡¬ç¼–ç è¿‡é•¿

**æ–‡ä»¶**: `src/web/app.py`  
**è¡Œå·**: 244-400

**é—®é¢˜æè¿°**:  
å¤§é‡æç¤ºè¯æ¨¡æ¿ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ï¼Œå ç”¨å¤§é‡è¡Œæ•°ä¸”éš¾ä»¥ç»´æŠ¤ã€‚

**å»ºè®®**: ç§»åˆ°å•ç‹¬çš„ YAML æˆ– JSON é…ç½®æ–‡ä»¶ã€‚

```yaml
# prompts/standard.yaml
name: "æ ‡å‡†åˆ†æï¼ˆæ¨èï¼‰"
system_prompt: |
  ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é‡‘èå¸‚åœºç ”ç©¶å‘˜...
analysis_prompt: |
  è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·...
```

```python
# åŠ è½½
import yaml

def load_prompt_templates():
    templates = {}
    prompts_dir = Path(__file__).parent / "prompts"
    for file in prompts_dir.glob("*.yaml"):
        with open(file) as f:
            template_id = file.stem
            templates[template_id] = yaml.safe_load(f)
    return templates
```

**ä¼˜å…ˆçº§**: ä½

---

### 4.4 é­”æ³•æ•°å­—ç¼ºä¹è¯´æ˜

**æ–‡ä»¶**: `src/ai_analyzer.py`  
**è¡Œå·**: 143, 182, 228

**é—®é¢˜æè¿°**:  
å¤šå¤„ç¡¬ç¼–ç æ•°å­—æ²¡æœ‰è¯´æ˜å«ä¹‰ã€‚

```python
analysis_text += f"å†…å®¹: {reply.main_content[:500]}\n"  # ä¸ºä»€ä¹ˆæ˜¯ 500?

for i, reply in enumerate(replies[:20]):  # ä¸ºä»€ä¹ˆæ˜¯ 20?

logger.debug(f"[AI API] Payload: {json.dumps(payload, ensure_ascii=False)[:500]}...")
```

**å»ºè®®**: ä½¿ç”¨å‘½åå¸¸é‡ã€‚

```python
MAX_CONTENT_LENGTH_PER_REPLY = 500  # å•æ¡å›å¤æœ€å¤§å­—ç¬¦æ•°ï¼Œæ§åˆ¶ token ä½¿ç”¨é‡
MAX_REPLIES_FOR_ANALYSIS = 20       # åˆ†ææ—¶æœ€å¤šä½¿ç”¨çš„å›å¤æ•°é‡
MAX_LOG_LENGTH = 500                # æ—¥å¿—æˆªæ–­é•¿åº¦
```

**ä¼˜å…ˆçº§**: ä½

---

### 4.5 æ—¥æœŸæ—¶é—´é»˜è®¤å€¼ä½¿ç”¨ lambda

**æ–‡ä»¶**: `src/db/models.py`  
**è¡Œå·**: 16-17

**é—®é¢˜æè¿°**:  
ä½¿ç”¨ `lambda: datetime.now(timezone.utc)` ä½œä¸ºé»˜è®¤å€¼ã€‚

**å½“å‰ä»£ç **:
```python
created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
```

**å»ºè®®**: ä½¿ç”¨ SQLAlchemy çš„ `func.now()` æˆ–ç¡®ä¿æ—¶åŒºä¸€è‡´ã€‚

```python
from sqlalchemy.sql import func

created_at = Column(DateTime, default=func.now())
# æˆ–
created_at = Column(DateTime(timezone=True), default=func.now())
```

**ä¼˜å…ˆçº§**: ä½

---

### 4.6 Webhook URL ç¼ºä¹éªŒè¯

**æ–‡ä»¶**: `src/discord_sender.py`  
**è¡Œå·**: 14

**é—®é¢˜æè¿°**:  
æ„é€ å‡½æ•°ç›´æ¥å­˜å‚¨ URLï¼Œæ²¡æœ‰éªŒè¯æ ¼å¼ã€‚

**å»ºè®®**:
```python
from urllib.parse import urlparse

class DiscordSender:
    def __init__(self, webhook_url: str):
        parsed = urlparse(webhook_url)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError("æ— æ•ˆçš„ webhook URL")
        if not webhook_url.startswith('https://discord.com/api/webhooks/'):
            logger.warning("Webhook URL ä¸æ˜¯æ ‡å‡†çš„ Discord webhook æ ¼å¼")
        self.webhook_url = webhook_url
```

**ä¼˜å…ˆçº§**: ä½

---

## 5. ä»£ç å¼‚å‘³

### 5.1 å‡½æ•°è¿‡é•¿

**æ–‡ä»¶**: `src/web/app.py`  
**é—®é¢˜**: `app.py` æ–‡ä»¶è¶…è¿‡ 700 è¡Œï¼ŒåŒ…å«å¤§é‡ç«¯ç‚¹å®šä¹‰ã€‚å»ºè®®æŒ‰åŠŸèƒ½æ‹†åˆ†åˆ°å¤šä¸ªè·¯ç”±æ–‡ä»¶ã€‚

```
src/web/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py          # FastAPI åº”ç”¨åˆå§‹åŒ–
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ targets.py   # ç›‘æ§ç›®æ ‡ç›¸å…³ç«¯ç‚¹
â”‚   â”œâ”€â”€ ai.py        # AI åˆ†æç›¸å…³ç«¯ç‚¹
â”‚   â”œâ”€â”€ archive.py   # å½’æ¡£ç›¸å…³ç«¯ç‚¹
â”‚   â”œâ”€â”€ schedule.py  # è°ƒåº¦è§„åˆ™ç›¸å…³ç«¯ç‚¹
â”‚   â””â”€â”€ config.py    # é…ç½®ç›¸å…³ç«¯ç‚¹
â””â”€â”€ templates/
```

---

### 5.2 ç±»èŒè´£ä¸å•ä¸€

**æ–‡ä»¶**: `src/web/app.py`  
**é—®é¢˜**: `AI_PROMPT_TEMPLATES` å­—å…¸åŒ…å«åœ¨ web æ¨¡å—ä¸­ï¼Œåº”è¯¥å±äº AI åˆ†ææ¨¡å—ã€‚

---

### 5.3 æ··åˆå…³æ³¨ç‚¹

**æ–‡ä»¶**: `src/monitor.py`  
**é—®é¢˜**: `check_and_send` å‡½æ•°åŒæ—¶å¤„ç†çˆ¬è™«ã€æ•°æ®åº“ã€Discord å‘é€ï¼ŒèŒè´£è¿‡é‡ã€‚å»ºè®®æ‹†åˆ†ä¸ºå¤šä¸ªå‡½æ•°ã€‚

---

## 6. å®‰å…¨å®¡è®¡ç»“æœ

| æ£€æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|--------|------|------|
| SQL æ³¨å…¥ | âœ… å®‰å…¨ | ä½¿ç”¨ SQLAlchemy ORMï¼Œå‚æ•°åŒ–æŸ¥è¯¢ |
| XSS | âš ï¸ éœ€æ³¨æ„ | è¾“å‡ºåˆ° Discord çš„å†…å®¹éœ€è¦è½¬ä¹‰ |
| SSRF | ğŸ”´ æ¼æ´ | `/api/ai/models` ç«¯ç‚¹å­˜åœ¨æ¼æ´ |
| æ•æ„Ÿä¿¡æ¯æ³„éœ² | ğŸ”´ æ¼æ´ | API Key æ³„éœ²åˆ°æ—¥å¿— |
| DoS | âš ï¸ éœ€æ³¨æ„ | limit å‚æ•°æ— ä¸Šé™ |
| è·¯å¾„éå† | âœ… å®‰å…¨ | ä½¿ç”¨ Path å¯¹è±¡ï¼Œç›¸å¯¹å®‰å…¨ |
| CSRF | âœ… ä¸é€‚ç”¨ | æ— çŠ¶æ€ APIï¼Œä½¿ç”¨ Token |

---

## é™„å½•ï¼šæ¨èçš„ä¾èµ–æ›´æ–°

```txt
# requirements.txt
# ç°æœ‰ä¾èµ–
fastapi>=0.100.0
sqlalchemy>=2.0.0
playwright>=1.40.0
requests>=2.31.0
jinja2>=3.1.0

# æ–°å¢ä¾èµ– - ç”¨äºå¼‚æ­¥ HTTP
aiohttp>=3.9.0
httpx>=0.25.0

# æ–°å¢ä¾èµ– - ç”¨äºé…ç½®ç®¡ç†
pydantic-settings>=2.0.0
pyyaml>=6.0.0

# æ–°å¢ä¾èµ– - ç”¨äºå®‰å…¨
python-ipware>=2.0.0  # IP åœ°å€éªŒè¯
```

---

## é™„å½•ï¼šé‡æ„ä¼˜å…ˆçº§è·¯çº¿å›¾

### Phase 1: å®‰å…¨ä¿®å¤ï¼ˆ1-2å¤©ï¼‰
1. ç§»é™¤ API Key æ—¥å¿—è®°å½•
2. ä¿®å¤ SSRF æ¼æ´
3. æ·»åŠ  limit å‚æ•°ä¸Šé™

### Phase 2: æ€§èƒ½ä¼˜åŒ–ï¼ˆ2-3å¤©ï¼‰
1. æ›¿æ¢ requests ä¸º aiohttp/httpx
2. å®ç°æ‰¹é‡æ•°æ®åº“æ“ä½œ
3. é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼

### Phase 3: ä»£ç è´¨é‡ï¼ˆ3-5å¤©ï¼‰
1. ç»Ÿä¸€æ•°æ®åº“ä¼šè¯ç®¡ç†
2. ä¿®å¤å¼‚å¸¸å¤„ç†
3. æå–é‡å¤ä»£ç 
4. æ‹†åˆ†å¤§å‹æ¨¡å—

### Phase 4: æ¶æ„ä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰
1. é…ç½®æ–‡ä»¶å¤–éƒ¨åŒ–
2. æ·»åŠ å•å…ƒæµ‹è¯•
3. å®ç°ä¾èµ–æ³¨å…¥
4. å®Œå–„ç›‘æ§å’Œæ—¥å¿—
